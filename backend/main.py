"""
Chat Educacional com Gemini AI
Backend API desenvolvido com FastAPI
"""

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional
import google.generativeai as genai
import os
from datetime import datetime
import logging
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import Response
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# M√©tricas Prometheus
REQUEST_COUNT = Counter('http_requests_total', 'Total de requisi√ß√µes HTTP', ['method', 'endpoint', 'status'])
REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'Lat√™ncia das requisi√ß√µes HTTP', ['endpoint'])
CHAT_MESSAGES = Counter('chat_messages_total', 'Total de mensagens do chat', ['topic'])
GEMINI_ERRORS = Counter('gemini_errors_total', 'Total de erros da API Gemini')

# Configurar FastAPI
app = FastAPI(
    title="Chat Educacional Gemini API",
    description="API de chat educacional em Cloud Computing usando Google Gemini",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    logger.error("GEMINI_API_KEY n√£o configurada!")
    raise ValueError("GEMINI_API_KEY n√£o encontrada nas vari√°veis de ambiente")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-pro')

# Models
class QuestionRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=1000, description="Pergunta do estudante")
    topic: Optional[str] = Field(None, description="T√≥pico da pergunta")
    conversation_id: Optional[str] = Field(None, description="ID da conversa para contexto")
    
    class Config:
        json_schema_extra = {
            "example": {
                "question": "O que √© Docker e para que serve?",
                "topic": "Docker e Containeriza√ß√£o"
            }
        }

class AnswerResponse(BaseModel):
    answer: str
    topic: Optional[str]
    timestamp: datetime
    conversation_id: str
    
class HealthResponse(BaseModel):
    status: str
    service: str
    version: str
    timestamp: datetime
    gemini_configured: bool

class Topic(BaseModel):
    id: str
    name: str
    description: str
    icon: str

class ErrorResponse(BaseModel):
    error: str
    detail: str
    timestamp: datetime

# T√≥picos dispon√≠veis
TOPICS = [
    Topic(
        id="docker",
        name="Docker e Containeriza√ß√£o",
        description="Conceitos de containers, imagens, volumes e redes",
        icon="üê≥"
    ),
    Topic(
        id="aws",
        name="AWS - Servi√ßos B√°sicos",
        description="EC2, S3, RDS, Lambda e outros servi√ßos fundamentais",
        icon="‚òÅÔ∏è"
    ),
    Topic(
        id="cicd",
        name="CI/CD e GitHub Actions",
        description="Integra√ß√£o e entrega cont√≠nuas, pipelines automatizados",
        icon="üîÑ"
    ),
    Topic(
        id="kubernetes",
        name="Kubernetes",
        description="Orquestra√ß√£o de containers, pods, services e deployments",
        icon="‚öì"
    ),
    Topic(
        id="terraform",
        name="Terraform e IaC",
        description="Infrastructure as Code, provisionamento automatizado",
        icon="üèóÔ∏è"
    ),
    Topic(
        id="security",
        name="Seguran√ßa em Cloud",
        description="IAM, VPC, Security Groups, SSL/TLS",
        icon="üîí"
    ),
    Topic(
        id="monitoring",
        name="Monitoramento e Logs",
        description="CloudWatch, Prometheus, Grafana, ELK Stack",
        icon="üìä"
    ),
    Topic(
        id="microservices",
        name="Arquitetura de Microservi√ßos",
        description="Design patterns, comunica√ß√£o entre servi√ßos, API Gateway",
        icon="üèõÔ∏è"
    ),
]

# Middleware para m√©tricas
@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    REQUEST_LATENCY.labels(endpoint=request.url.path).observe(duration)
    
    return response

# Health Check
@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["Health"],
    summary="Health check do servi√ßo"
)
async def health_check():
    """
    Verifica o status do servi√ßo e suas depend√™ncias
    """
    return HealthResponse(
        status="healthy",
        service="chat-educacional-gemini",
        version="1.0.0",
        timestamp=datetime.now(),
        gemini_configured=bool(GEMINI_API_KEY)
    )

# M√©tricas Prometheus
@app.get("/metrics", include_in_schema=False)
async def metrics():
    """Endpoint de m√©tricas para Prometheus"""
    return Response(content=generate_latest(), media_type="text/plain")

# Listar t√≥picos
@app.get(
    "/api/topics",
    response_model=List[Topic],
    tags=["Topics"],
    summary="Listar t√≥picos dispon√≠veis"
)
async def get_topics():
    """
    Retorna lista de t√≥picos educacionais dispon√≠veis
    """
    logger.info("Listando t√≥picos dispon√≠veis")
    return TOPICS

# Fazer pergunta
@app.post(
    "/api/ask",
    response_model=AnswerResponse,
    tags=["Chat"],
    summary="Fazer uma pergunta ao assistente",
    responses={
        200: {"description": "Resposta gerada com sucesso"},
        400: {"model": ErrorResponse, "description": "Requisi√ß√£o inv√°lida"},
        500: {"model": ErrorResponse, "description": "Erro no servidor"}
    }
)
async def ask_question(request: QuestionRequest):
    """
    Processa uma pergunta do estudante e retorna resposta do Gemini AI
    
    - **question**: Pergunta do estudante (obrigat√≥rio)
    - **topic**: T√≥pico relacionado (opcional)
    - **conversation_id**: ID para manter contexto da conversa (opcional)
    """
    try:
        # Log da pergunta
        logger.info(f"Nova pergunta recebida - T√≥pico: {request.topic}, Tamanho: {len(request.question)}")
        
        # Incrementar m√©trica
        topic_label = request.topic or "geral"
        CHAT_MESSAGES.labels(topic=topic_label).inc()
        
        # Criar prompt contextualizado
        prompt = create_educational_prompt(request.question, request.topic)
        
        # Chamar Gemini
        logger.info("Chamando API Gemini...")
        response = model.generate_content(prompt)
        
        if not response.text:
            logger.error("Gemini retornou resposta vazia")
            GEMINI_ERRORS.inc()
            raise HTTPException(
                status_code=500,
                detail="Gemini retornou resposta vazia"
            )
        
        answer = response.text
        logger.info(f"Resposta gerada com sucesso - Tamanho: {len(answer)}")
        
        # Gerar conversation_id se n√£o fornecido
        conversation_id = request.conversation_id or f"conv_{int(time.time())}"
        
        return AnswerResponse(
            answer=answer,
            topic=request.topic,
            timestamp=datetime.now(),
            conversation_id=conversation_id
        )
        
    except Exception as e:
        logger.error(f"Erro ao processar pergunta: {str(e)}")
        GEMINI_ERRORS.inc()
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao processar pergunta: {str(e)}"
        )

def create_educational_prompt(question: str, topic: Optional[str] = None) -> str:
    """
    Cria prompt educacional otimizado para Gemini
    """
    base_prompt = """Voc√™ √© um assistente educacional especializado em Cloud Computing, DevOps e AWS.

DIRETRIZES:
1. Responda de forma did√°tica e clara
2. Use exemplos pr√°ticos quando poss√≠vel
3. Explique conceitos t√©cnicos de forma acess√≠vel
4. Se a pergunta for muito ampla, foque nos pontos principais
5. Inclua boas pr√°ticas quando relevante
6. Use formata√ß√£o markdown para melhor legibilidade

"""
    
    if topic:
        base_prompt += f"\nT√ìPICO ESPEC√çFICO: {topic}\n"
    
    base_prompt += f"\nPERGUNTA DO ESTUDANTE: {question}\n\nRESPOSTA:"
    
    return base_prompt

# Endpoint raiz
@app.get("/", include_in_schema=False)
async def root():
    """Redirect para documenta√ß√£o"""
    return {
        "message": "Chat Educacional Gemini API",
        "docs": "/api/docs",
        "health": "/health",
        "version": "1.0.0"
    }

# Exception handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"Erro n√£o tratado: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Erro interno do servidor",
            "detail": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
